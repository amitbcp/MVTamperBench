<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Research Paper Showcase for Video-Language Models">
    <title>MVTamperBench</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&family=Poppins:wght@300;400;500&display=swap" rel="stylesheet">
    <style>
        /* General Styling */
        body {
            font-family: 'Poppins', Arial, sans-serif;
            margin: 0;
            padding: 0;
            color: #1e1e1e;
            background-color: #ffffff;
            overflow-x: hidden;
            text-align: center; /* Center alignment for the page */
        }

        header {
            background: #f7f7f7;
            padding: 4rem 2rem;
        }

        header h1 {
            font-size: 3rem;
            margin: 0;
            font-weight: 700;
            color: #181818;
        }

        header p {
            margin-top: 0.5rem;
            font-size: 1.2rem;
            font-weight: 500;
            color: #202020;
        }

        nav {
            margin-top: 2rem;
        }

        nav a {
            text-decoration: none;
            color: white;
            background: #005e91;
            padding: 1rem 2rem;
            border-radius: 50px;
            margin: 0 1rem;
            font-size: 1.1rem;
            transition: background 0.3s, transform 0.3s, box-shadow 0.3s;
        }

        nav a:hover {
            background: #001017;
            transform: scale(1.1);
            box-shadow: 0 4px 8px rgba(2, 132, 192, 0.949);
        }

        .hero {
            background: #dcdcdc;
            padding: 0.5rem;
            color: #202020;
            font-size: 1.6rem;
            border-radius: 10px;
            margin: 2rem auto;
            width: 75%;
            
        }

        main {
            padding: 3rem 2rem;
            max-width: 800px; /* Centralize main content */
            margin: 0 auto;
            
        }

        section {
            margin-bottom: 3rem;
        }

        section h2 {
            position: relative;
            padding-bottom: 0.5rem;
            margin-bottom: 1.2rem;
            font-size: 2.5rem;
            color: #181818;
            font-weight: 700;
        }

        section h2:after {
            content: "";
            position: absolute;
            bottom: 0;
            left: 50%;
            transform: translateX(-50%);
            width: 60px;
            height: 5px;
            background: #0077b6;
        }

        .content {
            text-align: justify;
            line-height: 1.8;
            font-size: 1.1rem;
            color: #444;
        }

        .links a {
            display: inline-block;
            text-decoration: none;
            color: white;
            background:#005e91;
            padding: 0.8rem 1.6rem;
            border-radius: 50px;
            margin: 1rem;
            font-size: 1.2rem;
            transition: background 0.3s, transform 0.3s;
        }

        .links a:hover {
            background: #001017;
            transform: scale(1.05);
            box-shadow:  0 4px 8px rgba(2, 132, 192, 0.949);
        }

        .line-design {
            text-align: center;
            margin: 3rem 0;
            color: #181818;
        }

        .line-design span {
            padding: 0 1.5rem;
            font-size: 1.8rem;
            color: #202020;
            font-weight: 800;
        }

        img {
            width: 70%;
            height: auto;
            margin: 1rem 0;
            border-radius: 10px; /* Optional for aesthetics */
        }

        .images{
            text-align: justify;
        }

        .Graphs{
            text-align: justify;
        }

        footer {
            background: #f7f7f7;
            color: #333;
            text-align: center;
            padding: 2rem 1rem;
            margin-top: 4rem;
        }

        footer a {
            color: #0077b6;
            text-decoration: none;
        }

        footer a:hover {
            text-decoration: underline;
        }

        .social-icons img {
            width: 40px;
            height: auto;
            margin: 0 1rem;
            transition: transform 0.3s;
        }

        .social-icons img:hover {
            transform: scale(1.1);
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            header h1 {
                font-size: 2.5rem;
            }

            header p {
                font-size: 1rem;
            }

            nav a {
                font-size: 0.9rem;
                padding: 0.8rem 1.5rem;
            }

            .hero {
                font-size: 1.2rem;
                padding: 1.5rem;
            }
        }

        @media (max-width: 480px) {
            nav a {
                margin: 0.5rem 0;
                display: block;
            }

            .hero {
                font-size: 1rem;
                padding: 1rem;
            }
        }
    </style>
</head>
<body>
    <header>
        <h1>MVTamperBench: Adversarial Benchmark for Evaluating the Robustness of Video Language Model</h1>
        <p>Comprehensive evaluation framework for robust video-language models</p>
        <nav>
            <a href="#abstract">Abstract</a>
            <a href="#images">Images</a>
            <a href="#graphs">Graphs</a>
            <a href="#leaderboard">Leaderboard</a>
            <a href="#links">Links</a>
        </nav>
    </header>

    <div class="hero">
        <p>Explore our benchmark, uncover insights, and evaluate cutting-edge VLM models.</p>
    </div>

    <main>
        <section id="abstract">
            <h2>Abstract</h2>
            <div class="content">
                <p>
                    Interaction between multiple modalities is an area that has received great attention in recent years. Video Language Models (VLMs) constitutes a relatively newer sub-domain of the existing ones that explore multimodal understanding, but are also quite powerful and have shown great potential. 
                    The main drawback of these models is that they can be disrupted by alterations in the video content, which do not necessarily require considerable effort. Therefore, in order to investigate the robustness of VLMs to real-world adversarial attacks, we present <strong>MVTamperBench</strong> that aims to fill in this gap. 
                    Our benchmark is based on MVBench dataset and implements five various techniques to try and introduce realistic alterations like substitution, masking, dropping, repeating, and rotating that may be encountered in real life scenarios including media editing, misinformation or even surveillance tampering.
                    We employed a tailored codebase for the purpose of handling the aforementioned modifications to MVBench videos with the aim of addressing diverse test cases to the VLMs. By testing state-of-the-art models against this benchmark, we accomplished our goals of understanding such manipulable content model weaknesses and strengths and the weaknesses and strengths of the manipulated models. 
                    According to our analysis, VLMs that are resistant to attempts in tampering still pose a reliable tool that is useful in adversarial situations remain in high demand. As a use case, we embedded the benchmark into VLMEvalKit, which we believe will pave way for future work focused on bolstering model robustness.
                </p>
            </div>
        </section>

        <div class="line-design"><span>Visuals</span></div>

        <section id="images">
            
            <h2>Images</h2>
            
            <img src="Tampering_Frames.png" alt="Example Frames">
            <div class = "images">
            <p>The above image gives us the clear idea of how these tampering effects are introduced into a video. 
                It shows us the comparison study of frames from the original video and their frames from respective tampering effects. 
                In this case, one frame is taken from each one second and was displayed for comparative study on how it works.</p>
            </div>
        </section>

        <section id="graphs">

            <h2>Graphs</h2>
            
            <img src="Distribution image.png" alt="Video Length Distribution Graph">
            <div class = "Graphs">
            <p>The above Distribution graph represents the distribution of the lengths of tampered videos in seconds.
                The x -axis implies the lengths of the videos within the range of 0 to 175 seconds. 
                The y-axis will represent the number of videos per length. This histogram shows that most of the videos are concentrated near 0.
                This means that most of the videos are relatively short. 
                As the video length increases, the number of videos decreases, with fewer longer videos observed.</p>
            </div>
            <img src="OverallPerformancebar.jpg" alt="Overall Performance of the Models">
            <div class = "Graphs">
                <p>The above Overall Model Performance graph shows the overall accuracy of different models, where InternVL2-8B outperforms all with the highest accuracy above 80%. Aria and Phi-3.5-Vision follow closely with slightly lower performance.
                 Models like Vintern-3B-beta and Llama-2-13B Vision-Instruct show moderate performance, while NVLM and VILA models exhibit relatively lower accuracies, with VILA-1.5-13B performing the least effectively.</p>
            </div>
            <img src="Heatmap_of_Effects.jpg" alt="Accuracy of Tampering Effects Across the Models">
            <div class = "Graphs">
            <p>The above heatmap indicates that InternVL2.8B is the strongest, with high accuracy held for all tampering effects while models such as VITL1.5-13b and VITL1.5-3b are highly susceptible to data tamperings.
                All tampering effects, including masking and substitution, tend to decrease the accuracy in most models, except that InternVL2.8B and especially PHI-3.5-Vision react slightly stronger. 
                This analysis therefore calls for model improvement, more so to address how well the models can cope with tampered data in such real-world applications.</p>
            </div>
        </section>

        <section id="leaderboard">
            <h2>Leaderboard</h2>
            <div class="links">
                <p>Check the leaderboard for the top-performing models on our benchmark.</p>
                <a href="https://huggingface.co/spaces/opencompass/open_vlm_leaderboard" target="_blank">View Leaderboard</a>
            </div>
        </section>

        <section id="links">
            <h2>Relevant Links</h2>
            <div class="links">
                <a href="https://github.com/amitbcp/TamperBench" target="_blank">GitHub Repository</a>
                <a href="https://huggingface.co/datasets/Srikant86/MVTamperBench/tree/main/video" target="_blank">Hugging Face Repo</a>
            </div>
        </section>
    </main>

    <footer>
        <p>&copy; 2024 VLM Benchmark. All rights reserved.</p>
        <div class="social-icons">
            <a href="https://github.com/amitbcp/TamperBench" target="_blank"><img src="https://logos-world.net/wp-content/uploads/2020/11/GitHub-Logo.png" alt="GitHub"></a>
            <a href="https://huggingface.co/datasets/Srikant86/MVTamperBench/tree/main/video" target="_blank"><img src="https://chunte-hfba.static.hf.space/images/Brand%20Logos/hf-logo-with-title.png" alt="Hugging Face"></a>
        </div>
    </footer>
</body>
</html>